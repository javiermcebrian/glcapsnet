# GLCapsNet

How does the visual representation of the world is structured by the brain? How could it be useful to react adaptively to new situations and scenarios? What if an autonomous system could learn that behavior?

GLCapsNet (Global-Local Capsule Network) is a Deep Learning architecture that is able to give **Context-Based Structure and Interpretability both Globally and Locally** for the Visual Attention task in Autonomous Driving scenarios. It makes use of **Capsule Networks** to achieve that goal.

Paper about this research is near to be published. Once this happens, code will be public at this repo.
For more information send an email to: **javiermcebrian@gmail.com**
